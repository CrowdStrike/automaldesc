# Created with https://citation-file-format.github.io/cff-initializer-javascript
cff-version: 1.2.0
title: >-
  AutoMalDesc: Large-Scale Script Analysis for Cyber Threat
  Research
message: >-
  If you use this software, please cite it using the
  metadata from this file.
type: software
authors:
  - family-names: Apostu
    given-names: Alexandru-Mihai
    affiliation: 'University of Bucharest, Romania & CrowdStrike'
    orcid: 'https://orcid.org/0009-0003-9437-5937'
  - family-names: Preda
    given-names: Andrei
    affiliation: CrowdStrike
    orcid: 'https://orcid.org/0009-0009-3139-4691'
  - family-names: Damir
    given-names: Alexandra Daniela
    affiliation: CrowdStrike
    orcid: 'https://orcid.org/0009-0007-7832-1911'
  - family-names: Bolocan
    given-names: Diana
    affiliation: CrowdStrike
    orcid: 'https://orcid.org/0009-0000-3602-0255'
  - family-names: Ionescu
    given-names: Radu Tudor
    affiliation: 'University of Bucharest, Romania'
    orcid: 'https://orcid.org/0000-0002-9301-1950'
  - family-names: Croitoru
    given-names: Ioana
    affiliation: CrowdStrike
    orcid: 'https://orcid.org/0000-0002-8015-9868'
  - family-names: GÄƒman
    given-names: Mihaela
    affiliation: CrowdStrike
    orcid: 'https://orcid.org/0000-0001-7751-6759'
repository-code: 'https://github.com/CrowdStrike/automaldesc'
abstract: >-
  Generating thorough natural language explanations for
  threat detections remains an open problem in cybersecurity
  research, despite significant advances in automated
  malware detection systems. In this work, we present
  AutoMalDesc, an automated static analysis summarization
  framework that, following initial training on a small set
  of expert-curated examples, operates independently at
  scale. This approach leverages an iterative self-paced
  learning pipeline to progressively enhance output quality
  through synthetic data generation and validation cycles,
  eliminating the need for extensive manual data annotation.
  Evaluation across 3,600 diverse samples in five scripting
  languages demonstrates statistically significant
  improvements between iterations, showing consistent gains
  in both summary quality and classification accuracy. Our
  comprehensive validation approach combines quantitative
  metrics based on established malware labels with
  qualitative assessment from both human experts and
  LLM-based judges, confirming both technical precision and
  linguistic coherence of generated summaries. To facilitate
  reproducibility and advance research in this domain, we
  publish our complete dataset of more than 100K script
  samples, including annotated seed (0.9K) and test (3.6K)
  datasets, along with our methodology and evaluation
  framework.
license: MIT
version: 1.0.0
date-released: '2026-01-20'
